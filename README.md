# CPSC-323-Lexical-Analysis-Project
  This repository contains a simple lexer implemented in C++ that tokenizes input code files based on predefined token patterns. The lexer scans through the input code and identifies various tokens such as identifiers, keywords, operators, separators, and real numbers.

# How to Use:

  We have used the online collaborative IDE Replit to construct our Lexer. If you would like to run it online without having to figure out the download, go to this link: https://replit.com/@kristianlosenar/CPSC-323-Lexical-Analysis-Project and sign in with either Google or Github. Once signed in, you can "fork and run" the code, in which you'll be able to run it from the website.

  If you want to run this in your own IDE, make sure you setup an environment with the following files: main.cpp, main.h, input_scode.txt, output.txt. From there you can follow the code run below. 

# CODE RUN:
When using the code, write any input in the input_scode.txt and click on main.cpp and run the code at the top. The file output.txt will update and will show you the list of tokens and their lexemes. 





# Input File Format
  The input code file should contain the source code that needs to be tokenized. Each line of code should be separated by newline characters.

# Output Format
  The output file output.txt will contain the tokens extracted from the input code along with their corresponding lexemes. Each token and lexeme pair will be listed in the following format: [TOKEN, LEXEME]





# Dependencies:
C++ Standard Library
# Acknowledgments
This lexer implementation was a school project for CSUF in Professor Modi's CPSC 323 Compilers & Languages
Special thanks to [Hunter Tran, Matthew Tran].
